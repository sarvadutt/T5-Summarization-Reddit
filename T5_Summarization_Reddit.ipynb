{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarvadutt/T5-Summarization-Reddit/blob/main/T5_Summarization_Reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLH7g5AQkpqL"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install transformers datasets evaluate rouge_score accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNQr1647ktkt"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer,T5ForConditionalGeneration\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch import optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XpnVLNZkxZO"
      },
      "outputs": [],
      "source": [
        "reddit = load_dataset('reddit_tifu', 'long')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jiSMgCT1cpUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6O9dw-27aMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd33f082-5156-4d63-f311-7aab71c06711"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ups': 115.0,\n",
              " 'num_comments': 23.0,\n",
              " 'upvote_ratio': 0.8799999952316284,\n",
              " 'score': 115.0,\n",
              " 'documents': 'this actually happened a couple of years ago. i grew up in germany where i went to a german secondary school that went from 5th to 13th grade (we still had 13 grades then, they have since changed that). my school was named after anne frank and we had a club that i was very active in from 9th grade on, which was dedicated to teaching incoming 5th graders about anne franks life, discrimination, anti-semitism, hitler, the third reich and that whole spiel. basically a day where the students\\' classes are cancelled and instead we give them an interactive history and social studies class with lots of activities and games. \\n\\nthis was my last year at school and i already had a lot of experience doing these project days with the kids. i was running the thing with a friend, so it was just the two of us and 30-something 5th graders. we start off with a brief introduction and brainstorming: what do they know about anne frank and the third reich? you\\'d be surprised how much they know. anyway after the brainstorming we do a few activities, and then we take a short break. after the break we split the class into two groups to make it easier to handle. one group watches a short movie about anne frank while the other gets a tour through our poster presentation that our student group has been perfecting over the years. then the groups switch. \\n\\ni\\'m in the classroom to show my group the movie and i take attendance to make sure no one decided to run away during break. i\\'m going down the list when i come to the name sandra (name changed). a kid with a boyish haircut and a somewhat deeper voice, wearing clothes from the boy\\'s section at a big clothing chain in germany, pipes up. \\n\\nnow keep in mind, these are all 11 year olds, they are all pre-pubescent, their bodies are not yet showing any sex specific features one would be able to see while they are fully clothed (e.g. boobs, beards,...). this being a 5th grade in the rather conservative (for german standards) bavaria, i was confused. i looked down at the list again making sure i had read the name right. look back up at the kid. \\n\\nme: \"you\\'re sandra?\"\\n\\nkid: \"yep.\"\\n\\nme: \"oh, sorry. *thinking the kid must be from somewhere where sandra is both a girl\\'s and boy\\'s name* where are you from? i\\'ve only ever heard that as a girl\\'s name before.\"\\n\\nthe class starts laughing. sandra gets really quiet. \"i am a girl...\" she says. some of the other students start saying that their parents made the same mistake when they met sandra. i feel so sorry and stupid. i get the class to calm down and finish taking attendance. we watch the movie in silence. after the movie, when we walked down to where the poster presentation took place i apologised to sandra. i felt so incredibly terrible, i still do to this day. throughout the rest of the day i heard lots of whispers about sandra. i tried to stop them whenever they came up, but there was no stopping the 5th grade gossip i had set in motion.\\n\\nsandra, if you\\'re out there, i am so incredibly sorry for humiliating you in front of your class. i hope you are happy and healthy and continue to live your life the way you like. don\\'t let anyone tell you you have to dress or act a certain way just because of the body parts you were born with. i\\'m sorry if i made you feel like you were wrong for dressing and acting differently. i\\'m sorry i probably made that day hell for you. i\\'m sorry for my ignorance.',\n",
              " 'tldr': 'confuse a 5th grade girl for a boy in front of half of her class. kids are mean. sorry sandra.**',\n",
              " 'title': 'gender-stereotyping'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "reddit[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3p4HiGl_886q"
      },
      "outputs": [],
      "source": [
        "# Load T5 tokenizer\n",
        "checkpoint = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "prefix = \"summarize: \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atS6JHP89AtL"
      },
      "outputs": [],
      "source": [
        "# Assuming 'documents', 'tldr', and 'title' are the relevant columns\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"documents\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"tldr\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahxp0s7A9pNK"
      },
      "outputs": [],
      "source": [
        "#  Tokenize the Reddit TIFU dataset\n",
        "tokenized_reddit = reddit.map(preprocess_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vENMMHxS-L-I"
      },
      "outputs": [],
      "source": [
        "#  Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA0fzD9T-Ods"
      },
      "outputs": [],
      "source": [
        "#  Load Rouge metric\n",
        "rouge = evaluate.load(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndI4-0xt-afh"
      },
      "outputs": [],
      "source": [
        "#  Compute metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agYleblD-gpT"
      },
      "outputs": [],
      "source": [
        "#  Load T5 base model\n",
        "model = T5ForConditionalGeneration.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing data & Loading"
      ],
      "metadata": {
        "id": "NdeECLNDDrDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "# Assuming 'text' is the key for the input data and 'summary' is the key for the target data\n",
        "input_key = 'documents'\n",
        "labels_key = 'tldr'\n",
        "\n",
        "# Assuming reddit[\"train\"] is your raw data\n",
        "#raw_data = reddit[\"train\"]\n",
        "raw_data = tokenized_reddit[\"train\"]\n",
        "\n",
        "# Use LabelEncoder to convert text labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform([item[labels_key] for item in raw_data])\n",
        "\n",
        "# Define a custom dataset\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, text, labels, max_length=1024):\n",
        "        self.text = text\n",
        "        self.labels = labels\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Tokenize the text with truncation and padding\n",
        "        inputs = tokenizer(\n",
        "            self.text[idx][input_key],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "\n",
        "            # check before run\n",
        "            'labels': inputs['input_ids'].squeeze().unsqueeze(0).clone()[:, 1:]\n",
        "\n",
        "        }\n",
        "\n",
        "# Assuming you have defined batch_size previously\n",
        "batch_size = 8\n",
        "\n",
        "# Create DataLoader for the full dataset\n",
        "full_dataset = MyDataset(raw_data, labels)\n",
        "full_data_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Clear unnecessary variables\n",
        "del raw_data\n",
        "gc.collect()\n",
        "del labels\n",
        "gc.collect()\n",
        "\n",
        "# Define the sizes for train and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "valid_size = len(full_dataset) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
        "\n",
        "# Clear unnecessary variables\n",
        "del full_dataset\n",
        "gc.collect()\n",
        "\n",
        "# Create DataLoader for training and validation sets\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Clear unnecessary variables\n",
        "del train_dataset\n",
        "gc.collect()\n",
        "del valid_dataset\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gacdTgqjDzHz",
        "outputId": "412ae0a3-f329-440f-b46a-4547ef3f0b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training and saving the model to local"
      ],
      "metadata": {
        "id": "U5HPvCqSD1Pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and loss function\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = torch.nn.KLDivLoss(reduction='batchmean', log_target=True)\n",
        "\n",
        "# Assuming you have defined the number of training epochs: num_epochs\n",
        "num_epochs = 4\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, batch in enumerate(train_data_loader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Prepare target distributions for label smoothing\n",
        "        target_dist = torch.full_like(logits, 0.1 / (logits.size(-1) - 1))\n",
        "        target_dist.scatter_(-1, input_ids.unsqueeze(-1), 0.9)\n",
        "\n",
        "        # Compute Kullback-Leibler Divergence loss\n",
        "        loss = criterion(F.log_softmax(logits, dim=-1), target_dist)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Accumulate gradients\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    # Validation loop (optional)\n",
        "    model.eval()\n",
        "    total_valid_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_data_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, labels=input_ids)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Prepare target distributions for label smoothing\n",
        "            target_dist = torch.full_like(logits, 0.1 / (logits.size(-1) - 1))\n",
        "            target_dist.scatter_(-1, input_ids.unsqueeze(-1), 0.9)\n",
        "\n",
        "            # Compute Kullback-Leibler Divergence loss\n",
        "            loss = criterion(F.log_softmax(logits, dim=-1), target_dist)\n",
        "            total_valid_loss += loss.item()\n",
        "\n",
        "    average_valid_loss = total_valid_loss / len(valid_data_loader)\n",
        "\n",
        "    # Print or log training/validation loss\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {average_valid_loss}\")\n",
        "\n",
        "\n",
        "\n",
        "model.save_pretrained(\"/content\")\n",
        "tokenizer.save_pretrained(\"/content\")"
      ],
      "metadata": {
        "id": "aFPZx2vSDzLo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6uE+5P+dEZ0DdcbyNxZ/X",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}